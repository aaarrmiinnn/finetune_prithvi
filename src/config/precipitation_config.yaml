# Precipitation Downscaling Configuration (ppt)
# Specialized configuration for training a model to predict precipitation only

# Data Configuration
data:
  merra2_dir: "data/merra2"
  prism_dir: "data/prism"
  dem_dir: "data/dem"
  dem_file: "PRISM_us_dem_4km_bil.bil"
  dates: ["20250302"]  # Only use March 2nd data
  spatial_extent: null  # null for whole domain, or [min_lon, min_lat, max_lon, max_lat]
  cache_dir: "cache"
  patch_size: 16  # Patch size for input data
  input_vars: ["T2MMAX", "T2MMEAN", "T2MMIN", "TPRECMAX"]  # Keep all inputs
  target_vars: ["ppt"]  # ONLY precipitation variable
  mask_ratio: 0.0  # No masking
  train_test_split: [0.7, 0.15, 0.15]  # Train, val, test split
  normalize_inputs: true  # Normalize inputs to [0, 1]
  normalize_targets: true  # Normalize targets to [0, 1]
  normalize_dem: true  # Normalize DEM to [0, 1]
  clip_extreme_values: true  # Clip values outside of [mean-3*std, mean+3*std]
  debug_mode: false  # Disable debug mode for production training

# Model Configuration
model:
  # Prithvi model configuration
  prithvi_checkpoint: "ibm-nasa-geospatial/Prithvi-WxC-1.0-2300M"  # Model name or path
  model_cache_dir: "models/cache"  # Directory to cache downloaded models
  use_pretrained: true  # Whether to use pretrained weights
  freeze_encoder: true  # Freeze the encoder to save memory
  
  # Architecture configuration - optimized for precipitation
  hidden_dim: 96  # Larger hidden dim for precipitation which needs more capacity

# Training Configuration
training:
  epochs: 15  # More epochs for precipitation which is harder to learn
  batch_size: 1  # Smaller batch size for more gradient updates
  max_nan_losses: 5  # Maximum number of NaN losses allowed before stopping
  gradient_clipping: true  # Enable gradient clipping 
  gradient_clip_val: 0.1  # Tighter gradient clipping for precipitation (more extreme values)
  precision: 16  # Use mixed precision for faster training
  accumulate_grad_batches: 8  # More gradient accumulation for effectively larger batch
  log_every_n_steps: 5  # Log more frequently to track the harder task
  val_check_interval: 0.25  # Run validation more frequently
  save_top_k: 3  # Save top 3 models
  resume_from_checkpoint: null  # Path to checkpoint to resume training
  debug: false  # Disable debug mode for production
  
  # Optimizer settings - optimized for precipitation
  optimizer:
    name: "AdamW"  # AdamW works well for this task
    lr: 3.0e-6  # Lower learning rate for more stable precipitation training
    weight_decay: 0.002  # Higher weight decay to prevent overfitting on sparse rain
    epsilon: 1e-8  # Epsilon parameter for AdamW optimizer
  
  # Scheduler settings
  scheduler:
    name: "cosine"  # Cosine annealing
    warmup_epochs: 3  # Longer warmup for stability with precipitation
    min_lr: 5e-8  # Lower minimum learning rate
    patience: 7  # More patience for ReduceLROnPlateau (precipitation is noisier)
    factor: 0.3  # Larger reduction factor

# Loss Configuration - MAE works better for precipitation
loss:
  mae_weight: 1.0  # Increased MAE weight for precipitation (better for sparse/skewed data)
  mse_weight: 0.1  # Reduced MSE weight for precipitation (sensitive to outliers)
  ssim_weight: 0.2  # Larger SSIM weight for better spatial pattern preservation

# Logging and Checkpoints
logging:
  save_dir: "logs"  # Directory to save logs
  name: "merra2-prism-precipitation-downscaling"  # Specific experiment name
  version: null  # Version (auto-increment if null)
  log_graph: true  # Log model graph
  tensorboard: true  # Enable TensorBoard logging
  
  # Weights & Biases configuration
  wandb: true  # Enable Weights & Biases logging
  wandb_project: "prithvi-downscaling"  # W&B project name
  wandb_entity: null  # W&B entity
  wandb_tags: ["prithvi", "downscaling", "merra2", "prism", "precipitation", "ppt"]  # W&B tags
  wandb_notes: "Precipitation-only (ppt) downscaling model"  # W&B notes
  
  # Debugging configuration
  debug: false  # Disable debug mode for production
  nan_debugging:
    enabled: true  # Keep NaN debugging enabled (important for precipitation)
    log_detailed_stats: true  # Log detailed tensor statistics
    log_layer_outputs: false  # Don't log outputs from each layer
    check_every_n_steps: 5  # Check model parameters more frequently

# Hardware Configuration
hardware:
  accelerator: "gpu"  # Use GPU for faster training
  devices: 1  # Number of devices to use
  num_workers: 4  # Increased workers for faster data loading
  pin_memory: true  # Enable pin memory for GPU
  device: "cuda"  # Device for model (cuda for NVIDIA GPUs)

# Cluster Configuration
cluster:
  enabled: false  # Disable cluster mode for single GPU
  strategy: "auto"  # Training strategy (auto, ddp, etc.)
  sync_batchnorm: true  # Synchronize batch normalization
  find_unused_parameters: false  # Find unused parameters 