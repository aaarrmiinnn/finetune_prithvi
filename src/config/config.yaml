# Data Configuration
data:
  # Data directories
  merra2_dir: "data/merra2"
  prism_dir: "data/prism"
  dem_dir: "data/dem"
  dem_file: "PRISM_us_dem_4km_bil.bil"
  cache_dir: "cache"

  # Data processing
  dates: []  # Empty list means use all available dates
  spatial_extent: null  # null for whole domain, or [min_lon, min_lat, max_lon, max_lat]
  patch_size: 32
  input_vars: ["T2MMAX", "T2MMEAN", "T2MMIN", "TPRECMAX"]
  target_vars: ["tdmean", "ppt"]
  mask_ratio: 0.15  # Ratio of input data to mask during training
  patch_stride: null  # If null, use patch_size
  
  # Data preprocessing
  train_test_split: [0.7, 0.15, 0.15]  # Train, val, test split
  normalize_inputs: true
  normalize_targets: true
  normalize_dem: true
  clip_extreme_values: true  # Clip values outside of [mean-3*std, mean+3*std]
  validate_data: true  # Enable data validation
  replace_nan_with_mean: true  # Replace NaNs with mean values

# Model Configuration
model:
  name: "Prithvi-WxC-1.0"
  prithvi_checkpoint: "ibm-nasa-geospatial/Prithvi-WxC-1.0-2300M"  # Model name or path
  cache_dir: "models/cache"
  use_pretrained: true
  freeze_encoder: true  # Freeze encoder by default for stability
  hidden_dim: 256
  device: "cuda"  # Will be overridden to "cpu" if CUDA is not available
  gradient_checkpointing: false  # Can be enabled for memory efficiency

# Training Configuration
training:
  # Basic training params
  epochs: 100
  batch_size: 32
  precision: 16  # Use mixed precision by default
  
  # Optimizer settings
  optimizer:
    name: "AdamW"
    lr: 1e-4
    weight_decay: 0.01
    epsilon: 1e-8
  
  # Learning rate scheduler
  scheduler:
    name: "cosine"
    warmup_epochs: 5
    min_lr: 1e-6
  
  # Training stability
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  log_every_n_steps: 50
  detect_anomaly: true  # Enable PyTorch anomaly detection
  
  # Checkpointing
  save_top_k: 3
  resume_from_checkpoint: null

# Loss Configuration
loss:
  mae_weight: 1.0
  mse_weight: 1.0
  ssim_weight: 0.1

# Logging Configuration
logging:
  save_dir: "logs"
  name: "merra2-prism-downscaling"
  version: null  # Auto-increment
  log_graph: true
  tensorboard: true
  wandb: true
  wandb_project: "merra2-prism-downscaling"
  wandb_entity: null
  wandb_tags: ["prithvi", "downscaling", "merra2", "prism"]
  wandb_notes: "Downscaling with frozen encoder"

# Hardware Configuration
hardware:
  accelerator: "auto"  # Let PyTorch decide based on availability
  devices: 1
  num_workers: 4
  pin_memory: true

# Distributed Training Configuration
distributed:
  enabled: false  # Enable for multi-GPU training
  strategy: "ddp"
  sync_batchnorm: true
  find_unused_parameters: false 