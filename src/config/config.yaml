# Data Configuration
data:
  merra2_dir: "data/merra2"
  prism_dir: "data/prism"
  dem_dir: "data/dem"
  dem_file: "PRISM_us_dem_4km_bil.bil"
  dates: ["20250302"]  # Only use March 2nd data
  spatial_extent: null  # null for whole domain, or [min_lon, min_lat, max_lon, max_lat]
  cache_dir: "cache"
  patch_size: 16  # Further reduced from 32 to save memory
  input_vars: ["T2MMAX", "T2MMEAN", "T2MMIN", "TPRECMAX"]
  target_vars: ["tdmean", "ppt"]  # PRISM variables (mean temperature and precipitation)
  mask_ratio: 0.0  # Removed masking for debugging
  train_test_split: [0.7, 0.15, 0.15]  # Train, val, test split
  debug_mode: true  # Added for debugging
  normalize_inputs: true  # Normalize inputs to [0, 1]
  normalize_targets: true  # Normalize targets to [0, 1]
  normalize_dem: true  # Normalize DEM to [0, 1]
  clip_extreme_values: true  # Clip values outside of [mean-3*std, mean+3*std]

# Model Configuration
model:
  # Prithvi model configuration
  prithvi_checkpoint: "ibm-nasa-geospatial/Prithvi-WxC-1.0-2300M"  # Model name or path
  cache_dir: "models/cache"  # Directory to cache downloaded models
  use_pretrained: true  # Whether to use pretrained weights
  device: "cpu"  # Use CPU to avoid MPS memory issues
  freeze_encoder: true  # Freeze the encoder to save memory
  
  # Architecture configuration
  hidden_dim: 32  # Further reduced from 64 to save memory
  
  # Training configuration
  learning_rate: 5e-6  # Further reduced for stability
  weight_decay: 1e-5   # Increased for better regularization
  scheduler_patience: 5
  scheduler_factor: 0.5
  loss_weights:
    mse: 1.0
    mae: 1.0

# Training Configuration
training:
  epochs: 5  # Increased epochs for better convergence
  batch_size: 1  # Reduced to minimum to save memory
  max_nan_losses: 5  # Maximum number of NaN losses allowed before stopping
  gradient_clipping: true  # Enable gradient clipping
  optimizer:
    name: "AdamW"
    lr: 5.0e-6  # Reduced learning rate for stability
    weight_decay: 0.002  # Increased weight decay
    epsilon: 1e-8  # Epsilon parameter for AdamW optimizer
  scheduler:
    name: "cosine"
    warmup_epochs: 1  # Add warmup for stability
    min_lr: 1e-7  # Minimum learning rate
  precision: 32  # Use full precision for debugging
  gradient_clip_val: 0.1  # Reduced for stability
  log_every_n_steps: 1  # Log every step
  val_check_interval: 0.5  # Run validation more frequently
  save_top_k: 2  # Save top 2 models
  resume_from_checkpoint: null  # Path to checkpoint to resume training
  debug: true  # Added for debugging
  accumulate_grad_batches: 4  # Increased to accumulate more gradients

# Loss Configuration
loss:
  mae_weight: 1.0
  mse_weight: 0.1  # Reduced MSE contribution
  ssim_weight: 0.0  # Disabled SSIM for debugging

# Logging and Checkpoints
logging:
  save_dir: "logs"
  name: "merra2-prism-downscaling-debug"
  version: null  # Auto-increment
  log_graph: true
  tensorboard: true
  wandb: false  # Disabled for debugging
  wandb_project: "merra2-prism-downscaling"
  wandb_entity: null
  wandb_tags: ["prithvi", "downscaling", "merra2", "prism", "debug"]
  wandb_notes: "Debugging run to identify NaN values"
  nan_debugging:
    enabled: true  # Enable special NaN debugging logs
    log_detailed_stats: true  # Log detailed tensor statistics
    log_layer_outputs: false  # Don't log outputs from each layer (can be large)
    check_every_n_steps: 5  # Check model parameters every N steps

# Hardware
hardware:
  accelerator: "cpu"  # Use CPU to avoid MPS memory issues
  devices: 1
  num_workers: 0  # Single-threaded loading for debugging
  pin_memory: false  # Disable pin_memory when using CPU
  
# Cluster Settings (for future use)
cluster:
  enabled: false
  distributed_backend: "ddp"
  sync_batchnorm: true
  strategy: "ddp"
  find_unused_parameters: false 