# Data Configuration
data:
  merra2_dir: "data/merra2"
  prism_dir: "data/prism"
  dem_dir: "data/dem"
  dem_file: "PRISM_us_dem_4km_bil.bil"
  dates: ["20250302"]  # Only use March 2nd data
  spatial_extent: null  # null for whole domain, or [min_lon, min_lat, max_lon, max_lat]
  cache_dir: "cache"
  patch_size: 32  # Reduced from 64 to save memory
  input_vars: ["T2MMAX", "T2MMEAN", "T2MMIN", "TPRECMAX"]
  target_vars: ["tdmean", "ppt"]  # PRISM variables (mean temperature and precipitation)
  mask_ratio: 0.3  # Ratio of pixels to mask for training
  train_test_split: [0.7, 0.15, 0.15]  # Train, val, test split

# Model Configuration
model:
  # Prithvi model configuration
  prithvi_checkpoint: "ibm-nasa-geospatial/Prithvi-WxC-1.0-2300M"  # Model name or path
  cache_dir: "models/cache"  # Directory to cache downloaded models
  use_pretrained: true  # Whether to use pretrained weights
  device: "mps"  # Use MPS device for Mac; use "cuda" for GPU cluster
  freeze_encoder: false  # Whether to freeze the encoder during training
  
  # Architecture configuration
  hidden_dim: 128  # Reduced size for memory efficiency (was 256)
  
  # Training configuration
  learning_rate: 1e-4
  weight_decay: 1e-6
  scheduler_patience: 5
  scheduler_factor: 0.5
  loss_weights:
    mse: 1.0
    mae: 1.0

# Training Configuration
training:
  epochs: 30
  batch_size: 1  # Reduced from 2 to save memory
  optimizer:
    name: "AdamW"
    lr: 1.0e-4
    weight_decay: 0.01
  scheduler:
    name: "cosine"
    warmup_epochs: 3
  precision: 32  # Use full precision on CPU; use 16 for GPU cluster for faster training
  gradient_clip_val: 1.0
  log_every_n_steps: 1  # Reduced from 50 since we have fewer batches
  val_check_interval: 1.0  # Fraction of epoch to run validation
  save_top_k: 3  # Number of best checkpoints to save
  resume_from_checkpoint: null  # Path to checkpoint to resume training

# Loss Configuration
loss:
  mae_weight: 1.0
  mse_weight: 0.5
  ssim_weight: 0.2  # Structural similarity index measure

# Logging and Checkpoints
logging:
  save_dir: "logs"
  name: "merra2-prism-downscaling"
  version: null  # Auto-increment
  log_graph: true
  tensorboard: true
  wandb: true  # Enable Weights & Biases logging
  wandb_project: "merra2-prism-downscaling"
  wandb_entity: null  # Your wandb username or team name
  wandb_tags: ["prithvi", "downscaling", "merra2", "prism"]
  wandb_notes: "Initial training run on local machine"

# Hardware
hardware:
  accelerator: "mps"  # Use MPS (Metal Performance Shaders) on Mac; change to "gpu" for GPU cluster
  devices: 1
  num_workers: 1  # Reduced from 2 to save memory
  pin_memory: true  # Enable pin_memory for GPU
  
# Cluster Settings (for future use)
cluster:
  enabled: false  # Set to true when running on cluster
  distributed_backend: "ddp"  # Distributed Data Parallel
  sync_batchnorm: true
  strategy: "ddp"
  find_unused_parameters: false 