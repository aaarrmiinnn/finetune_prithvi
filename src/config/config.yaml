# Clean Configuration File for MERRA-2 to PRISM Downscaling

# Data Configuration
data:
  merra2_dir: "data/merra2"
  prism_dir: "data/prism"
  dem_dir: "data/dem"
  dem_file: "PRISM_us_dem_4km_bil.bil"
  dates: ["20250302"]  # Only use March 2nd data
  spatial_extent: null  # null for whole domain, or [min_lon, min_lat, max_lon, max_lat]
  cache_dir: "cache"
  patch_size: 16  # Patch size for input data
  input_vars: ["T2MMAX", "T2MMEAN", "T2MMIN", "TPRECMAX"]
  target_vars: ["tdmean", "ppt"]  # PRISM variables (mean temperature and precipitation)
  mask_ratio: 0.0  # Masking ratio for training
  train_test_split: [0.7, 0.15, 0.15]  # Train, val, test split
  normalize_inputs: true  # Normalize inputs to [0, 1]
  normalize_targets: true  # Normalize targets to [0, 1]
  normalize_dem: true  # Normalize DEM to [0, 1]
  clip_extreme_values: true  # Clip values outside of [mean-3*std, mean+3*std]
  debug_mode: true  # Added for debugging (keeping for backward compatibility)

# Model Configuration
model:
  # Prithvi model configuration
  prithvi_checkpoint: "ibm-nasa-geospatial/Prithvi-WxC-1.0-2300M"  # Model name or path
  model_cache_dir: "models/cache"  # Directory to cache downloaded models
  use_pretrained: true  # Whether to use pretrained weights
  freeze_encoder: true  # Freeze the encoder to save memory
  
  # Architecture configuration
  hidden_dim: 32  # Hidden dimension size for the model

# Training Configuration
training:
  epochs: 5  # Number of training epochs
  batch_size: 1  # Batch size for training
  max_nan_losses: 5  # Maximum number of NaN losses allowed before stopping
  gradient_clipping: true  # Enable gradient clipping
  gradient_clip_val: 0.1  # Gradient clipping value
  precision: 32  # Precision for training (16, 32, 64)
  accumulate_grad_batches: 4  # Number of batches to accumulate gradients
  log_every_n_steps: 1  # Log frequency
  val_check_interval: 0.5  # Validation check interval
  save_top_k: 2  # Number of best models to save
  resume_from_checkpoint: null  # Path to checkpoint to resume training
  debug: true  # Debug mode
  
  # Optimizer settings
  optimizer:
    name: "AdamW"  # Optimizer name
    lr: 5.0e-6  # Learning rate
    weight_decay: 0.002  # Weight decay
    epsilon: 1e-8  # Epsilon parameter for AdamW optimizer
  
  # Scheduler settings
  scheduler:
    name: "cosine"  # Scheduler name
    warmup_epochs: 1  # Number of warmup epochs
    min_lr: 1e-7  # Minimum learning rate
    patience: 5  # Patience for ReduceLROnPlateau
    factor: 0.5  # Factor for ReduceLROnPlateau

# Loss Configuration
loss:
  mae_weight: 1.0  # Weight for MAE loss
  mse_weight: 0.1  # Weight for MSE loss
  ssim_weight: 0.0  # Weight for SSIM loss

# Logging and Checkpoints
logging:
  save_dir: "logs"  # Directory to save logs
  name: "merra2-prism-downscaling-debug"  # Experiment name
  version: null  # Version (auto-increment if null)
  log_graph: true  # Log model graph
  tensorboard: true  # Enable TensorBoard logging
  
  # Weights & Biases configuration
  wandb: false  # Enable Weights & Biases logging
  wandb_project: "merra2-prism-downscaling"  # W&B project name
  wandb_entity: null  # W&B entity
  wandb_tags: ["prithvi", "downscaling", "merra2", "prism", "debug"]  # W&B tags
  wandb_notes: "Debugging run to identify NaN values"  # W&B notes
  
  # Debugging configuration
  debug: false  # Enable debugging mode
  nan_debugging:
    enabled: true  # Enable NaN debugging
    log_detailed_stats: true  # Log detailed tensor statistics
    log_layer_outputs: false  # Log outputs from each layer
    check_every_n_steps: 5  # Check model parameters every N steps

# Hardware Configuration
hardware:
  accelerator: "cpu"  # Accelerator type (cpu, gpu, mps)
  devices: 1  # Number of devices to use
  num_workers: 0  # Number of workers for data loading
  pin_memory: false  # Pin memory for data loading
  device: "cpu"  # Device for model (cpu, cuda, mps)

# Cluster Configuration
cluster:
  enabled: false  # Enable cluster mode
  strategy: "auto"  # Training strategy (auto, ddp, etc.)
  sync_batchnorm: true  # Synchronize batch normalization
  find_unused_parameters: false  # Find unused parameters 