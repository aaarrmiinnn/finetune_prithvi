# Temperature Downscaling Configuration (tdmean)
# Specialized configuration for training a model to predict temperature only

# Data Configuration
data:
  merra2_dir: "data/merra2"
  prism_dir: "data/prism"
  dem_dir: "data/dem"
  dem_file: "PRISM_us_dem_4km_bil.bil"
  dates: ["20250302"]  # Only use March 2nd data
  spatial_extent: null  # null for whole domain, or [min_lon, min_lat, max_lon, max_lat]
  cache_dir: "cache"
  patch_size: 16  # Patch size for input data
  input_vars: ["T2MMAX", "T2MMEAN", "T2MMIN", "TPRECMAX"]  # Keep all inputs
  target_vars: ["tdmean"]  # ONLY temperature variable
  mask_ratio: 0.0  # No masking
  train_test_split: [0.7, 0.15, 0.15]  # Train, val, test split
  normalize_inputs: true  # Normalize inputs to [0, 1]
  normalize_targets: true  # Normalize targets to [0, 1]
  normalize_dem: true  # Normalize DEM to [0, 1]
  clip_extreme_values: true  # Clip values outside of [mean-3*std, mean+3*std]
  debug_mode: false  # Disable debug mode for production training

# Model Configuration
model:
  # Prithvi model configuration
  prithvi_checkpoint: "ibm-nasa-geospatial/Prithvi-WxC-1.0-2300M"  # Model name or path
  model_cache_dir: "models/cache"  # Directory to cache downloaded models
  use_pretrained: true  # Whether to use pretrained weights
  freeze_encoder: true  # Freeze the encoder to save memory
  
  # Architecture configuration - can be larger for temperature only
  hidden_dim: 64  # Increased from 32 for better temperature prediction

# Training Configuration
training:
  epochs: 10  # Increased epochs for better convergence
  batch_size: 2  # Increased batch size since we're training for one variable
  max_nan_losses: 5  # Maximum number of NaN losses allowed before stopping
  gradient_clipping: true  # Enable gradient clipping
  gradient_clip_val: 0.5  # Increased for temperature which tends to be smoother
  precision: 16  # Use mixed precision for faster training
  accumulate_grad_batches: 4  # Accumulate gradients for effectively larger batch
  log_every_n_steps: 10  # Log less frequently for faster training
  val_check_interval: 0.5  # Run validation frequently
  save_top_k: 3  # Save top 3 models
  resume_from_checkpoint: null  # Path to checkpoint to resume training
  debug: false  # Disable debug mode for production
  
  # Optimizer settings - optimized for temperature
  optimizer:
    name: "AdamW"  # AdamW works well for this task
    lr: 1.0e-5  # Increased learning rate for temperature
    weight_decay: 0.001  # Reduced weight decay for temperature
    epsilon: 1e-8  # Epsilon parameter for AdamW optimizer
  
  # Scheduler settings
  scheduler:
    name: "cosine"  # Cosine annealing
    warmup_epochs: 2  # Longer warmup for stability
    min_lr: 1e-7  # Minimum learning rate
    patience: 5  # Patience for ReduceLROnPlateau
    factor: 0.5  # Factor for ReduceLROnPlateau

# Loss Configuration - MSE works well for temperature
loss:
  mae_weight: 0.5  # Reduced MAE weight for temperature
  mse_weight: 1.0  # Increased MSE weight for temperature (better for continuous values)
  ssim_weight: 0.1  # Add small SSIM weight for spatial consistency

# Logging and Checkpoints
logging:
  save_dir: "logs"  # Directory to save logs
  name: "merra2-prism-temperature-downscaling"  # Specific experiment name
  version: null  # Version (auto-increment if null)
  log_graph: true  # Log model graph
  tensorboard: true  # Enable TensorBoard logging
  
  # Weights & Biases configuration
  wandb: true  # Enable Weights & Biases logging
  wandb_project: "prithvi-downscaling"  # W&B project name
  wandb_entity: null  # W&B entity
  wandb_tags: ["prithvi", "downscaling", "merra2", "prism", "temperature", "tdmean"]  # W&B tags
  wandb_notes: "Temperature-only (tdmean) downscaling model"  # W&B notes
  
  # Debugging configuration
  debug: false  # Disable debug mode for production
  nan_debugging:
    enabled: true  # Keep NaN debugging enabled
    log_detailed_stats: true  # Log detailed tensor statistics
    log_layer_outputs: false  # Don't log outputs from each layer
    check_every_n_steps: 10  # Check model parameters less frequently

# Hardware Configuration
hardware:
  accelerator: "gpu"  # Use GPU for faster training
  devices: 1  # Number of devices to use
  num_workers: 4  # Increased workers for faster data loading
  pin_memory: true  # Enable pin memory for GPU
  device: "cuda"  # Device for model (cuda for NVIDIA GPUs)

# Cluster Configuration
cluster:
  enabled: false  # Disable cluster mode for single GPU
  strategy: "auto"  # Training strategy (auto, ddp, etc.)
  sync_batchnorm: true  # Synchronize batch normalization
  find_unused_parameters: false  # Find unused parameters 